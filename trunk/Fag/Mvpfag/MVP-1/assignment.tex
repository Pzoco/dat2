\documentclass{article}
\usepackage{a4wide}
\usepackage{listings,color}
\usepackage[lastexercise]{exercise}
\renewcommand{\ExerciseListHeader}{ 
\textbf{Exercise} \ExerciseHeaderNB:\quad} 
\renewcommand{\QuestionNB}{{\bf Q}:\quad}
\renewcommand{\AnswerListHeader}{ \textbf{A}:\quad} 
\pagestyle{empty}

\title{MVP Assignment 1}
\date{Due date: 14/3/2010}
\begin{document}
\maketitle

% Comment this for answering.
%\newcommand{\question}[1]{#1}
%\newcommand{\answer}[1]{}

% Uncomment this for answering.
\newcommand{\question}[1]{}
\newcommand{\answer}[1]{{#1}}

%
% Write your group room and group number here.
%
\answer{
\begin{flushleft}
{\bf Group room: 2.1.57}
{\bf Group number: D204A}
\end{flushleft}
}

\section{Question}

\begin{ExerciseList}
\Exercise Exercise in the book.
\Question \emph{Exercise 1, chapter 1.}
\Answer
\newline
\begin{itemize}

	\item Thread: When dividing a program, threads can be used to run the program in parallel. You might say that a thread is a unit of execution.
	\item Race condition: A race condition occurs when two or more threads race to influence something. When the threads try to influence something at the same time, this is dependant on the interleaving of these threads, and a race condition may raise problems in certain situations.
	\item Mutex: A mutex is used to lock some lines of code so that only one thread of a time is allowed to execute the lines. If we want to have some critical section of a code executed atomically we use a mutex to make the threads mutually exclude each other. This means that once one thread enters the mutex, it locks it, and no other thread will enter this section until the other has finished its work and unlocks the mutex.
	\item Lock Contention: This occurs when two or more threads tries to acquire a lock at the same time, or trying to acquire a lock that should belong to another thread.
	\item Granularity: Is the size of the subproblems. When talking about granularity if the subproblems are larger we say the problem is more 'coarse'.
	\item False Sharing: If two threads shares a cache line, they still need to reload the cache line every time they try to access their value in case that 
	the other thread might have changed something in the cache line. In a sense they are sharing the same data but still reloading the cacheline individually. their value in case that 
	the other thread might have changed something in the cache line.
\end{itemize}

\end{ExerciseList}

\section{Matrix Multiplication}

\begin{ExerciseList}
\Exercise
\answer{Matrix multiplication.}
\question{
In this first assignment we will get warmed-up with C and experiment
with the influence of caches on an apparently simple algorithm. The
standard formula for computing the matrix multiplication $C=A*B$
between two matrices of size $n*n$\footnote{We stick to square
matrices for simplicity here.} is:

\[ (c_{ij})_{1\le i,j \le n} = \sum_{k=1}^{n} a_{ik}*b_{kj} \]

\Question \emph{Write down the straight-forward algorithm for
computing this formula.}
% You do not need to give me this algorithm here.
}

\Question \emph{Implement this algorithm in matrix\_fibo.c.}
\question{This program serves
as a test program to help you develop and test your function. It is
computing Fibonacci numbers using two techniques and it compares the
results for correctness. Keen students are invited to look at the
slides that explain the example (given in the source code). You do
not need to look at this to complete the assignment.

Notes: \texttt{const} means you are not supposed to write the data
(see the source code). To address element $(i,j)$ in a matrix of
dimention $n$ declared as \texttt{int* a}, use \texttt{a[i*dim+j]}.}

% Place your function here.
\begin{lstlisting}[basicstyle=\small\sffamily,
keywords={break,case,const,continue,default,else,enum,
for,if,return,switch,while,do,long,void,int,float,double,
char,struct,typedef,include,size\_t},
keywordstyle={\color{blue}},
comment={[l]{//}}, morecomment={[s]{/*}{*/}}, commentstyle=\itshape,
columns={[l]flexible}, numbers=left, numberstyle=\tiny,
frameround=fftt, frame=shadowbox, captionpos=b,
caption={Matrix multiplication function.},
label=LST:MatMulFunc1]

void mat_mult1(double *a, double *b, double *c, size_t n)
{
    size_t i, j, k;
    assert(a != b && a != c);

    for(i= 0;i<n;i++)
	{
		for(j = 0;j<n;j++)
		{
		double temp = 0;
			for(k = 0;k<n;k++)
			{		
        	        temp+= a[i*n+k]*b[k*n+j];
			}
            	c[i*n+j] +=temp;
		}
	}
}
\end{lstlisting}
\end{ExerciseList}

\section{Re-arranged Matrix-Multiplication}

\begin{ExerciseList}
\Exercise
\answer{Studying the cache influence.}
\question{
You will test subsequent matrix-multiplication algorithms using
another test program. It is basically generating a random matrix,
inverting it, then it multiplies the original with its inverse and
checks that it is the identity.

\Question
\emph{Transfer your matrix multiplication implementation to pmatrix.c
  in the mat\_mult1 function.} You will need to change the data type to
  double.
% You don't need to give me this code.
}

\Question
\emph{Why is the identity check not using simple equality tests
  like $x==0$ or $x==1.0$?}
\Answer The simple equality checks determine equality on a per-byte basis. If we simply compare whether or not they are equal in byte-sizes, the double may still be considered to be larger since more decimals are stored in the memory.

\Question
\emph{Change the PIVOT parameter to 0. Explain the loss in precision.}
Turn back the parameter to 1 after that question.
\Answer When PIVOT is set to 0, we don't use pivoting in the Gaussian elimination, therefore we have a risk of numerical instability e.g. when dividing with
small values.

\Question
\emph{What is the memory access pattern of your matrix multiplication
  implementation?}
\Answer The a matrix is read sequential and the b matrix is read column wise.

\Question
\emph{Why is it bad for the cache?}
\Answer The cache is written to and read in strides, therefore we need to change the values in the cache more often, which leads to more operations on a hardware-level. This is bad for the cache because it is unnecessary to update it so frequently.

\Question
\emph{Modify your matrix multiplication implementation to make it more
  cache friendly.}
\question{This modification should only concern the memory access
  pattern. One very simple change in the loops is enough.}

% Place your function here.
\begin{lstlisting}[basicstyle=\small\sffamily,
keywords={break,case,const,continue,default,else,enum,
for,if,return,switch,while,do,long,void,int,float,double,
char,struct,typedef,include,size\_t},
keywordstyle={\color{blue}},
comment={[l]{//}}, morecomment={[s]{/*}{*/}}, commentstyle=\itshape,
columns={[l]flexible}, numbers=left, numberstyle=\tiny,
frameround=fftt, frame=shadowbox, captionpos=b,
caption={Matrix multiplication function.},
label=LST:MatMulFunc2]

void mat_mult2(const double* a, const int* b, int* c, size_t n)
{
    int i, j, k;              /* You will need at least these. */
    assert(a != c && b != c); /* Check precondition. */

    /* Write here your matrix multiplication. */
    for(i = 0;i<n;i++)
    {
        for(k= 0;k<n;k++)
        {
            double temp = 0;
            for(j = 0;j<n;j++)
            {
                temp+= a[i*n+k]*b[k*n+j];
                c[i*n+j] +=temp;
	    }
	    }
	}
}
\end{lstlisting}

\Question
\emph{Test your program with sizes 100, 200, \dots 1000} and report
the relative improvement compared to the previous version.
\Answer 
\begin{tabular}{l*{2}{r}c}
MSize            & Mult1	& Mult2	& Improvement (\%) \\
\hline
100 			& 0.014938s		& 0.023542s		& $\sim$ -36.57 \\
200            	& 0.21661s		& 0.190523s		& $\sim$ 4.12 \\
1000     		& 37.6788s		& 23.7741s		& $\sim$ 58.48	\\
\end{tabular} \\

\end{ExerciseList}

\section{Block-Matrix Multiplication}

\begin{ExerciseList}
\Exercise
\answer{Block matrix multiplication.}
\question{
We want to improve our implementation to make it more cache
friendly. The idea of the block-matrix multiplication is to compute
the multiplication not by element-wise multiplications but by
blocks-wise multiplications. Thus the formula becomes:

\[ (C_{ij})_{1\le i,j \le n} = \sum_{k=1}^{n} A_{ik}*B_{kj} \]

\noindent
where $A_{ij}, B_{ij},$ and $C_{ij}$ are block sub-matrices of
respectively $A, B,$ and $C$. The multiplication between blocks is the
standard element-wise multiplication. We note that thanks to the
associativity of additions this formulation is equivalent to the
original algorithm.}

\Question
\emph{What is the point of this reformulation?}
\Answer By calculating on the blocks of the matrix in the cache, rather than the sequentially listed matrix, the calculation becomes more cache-friendly because the data in the cache is not changed as often.

\question{
\Question
\emph{Write down the corresponding algorithm.}
% You do not need to give me this algorith, I will see it in your code.
}

\Question
\emph{Implement this algorithm.} Use the BLOCK parameter to tune the
size of the blocks.

% Place your function here.
\begin{lstlisting}[basicstyle=\small\sffamily,
keywords={break,case,const,continue,default,else,enum,
for,if,return,switch,while,do,long,void,int,float,double,
char,struct,typedef,include,size\_t},
keywordstyle={\color{blue}},
comment={[l]{//}}, morecomment={[s]{/*}{*/}}, commentstyle=\itshape,
columns={[l]flexible}, numbers=left, numberstyle=\tiny,
frameround=fftt, frame=shadowbox, captionpos=b,
caption={Matrix multiplication function.},
label=LST:MatMulFunc3]

void mat_mult3(double *a, double *b, double *c, size_t n)
{
    size_t bi, bj, bk, i, j, k, maxi, maxj, maxk;

    for(bi = 0;bi<n;bi+=BLOCK)
    {
        maxi = min(bi+BLOCK,maxi);
        for(bj = 0;bj<n;bj+=BLOCK)
        {
            maxj = min(bj+BLOCK,maxj);
            for(bk = 0;bk < n;bk+=BLOCK)
            {
                maxk = min(bk+BLOCK,maxk);
                for(i = 0;i<maxi;i++)
                {
                    for(j= 0;j<maxj;j++)
                    {
                        double temp = 0;
                        for(k = 0;k<maxk;k++)
                        {
                        temp+= a[i*n+k]*b[k*n+j];
                        }
                        c[i*n+j] +=temp;
                    }
                }
            }
        }
    }
}

\end{lstlisting}

\Question
\emph{Experiment with the BLOCK parameter and find a good value for
  your machine. Why is it a good value for your machine?}
\Answer By experimenting we found that setting the Block size to 40 we achieved the greatest improvement. Actually the improvement from just 39 or 41 was rather staggering. The main idea here is to fit the block in the cache. We are unsure of the cache size on the application server.

\Question
\emph{Test your program with sizes 100, 200, \dots 1000} and report
the relative improvement compared to the previous version.
\Answer 
\begin{tabular}{l*{2}{r}c}
MSize            & Mult2	& Mult3	& Improvement (\%) \\
\hline
100 			& 0.023542s		& 0.026063s		& $\sim$ -9.67 \\
200            	& 0.190523s		& 0.119536s		& $\sim$ 59.38 \\
1000     		& 23.7741s		& 16.373s		& $\sim$ 45.20	\\
\end{tabular} \\

\Exercise $[$Optional$]$ Micro benchmarks are relatively small
benchmark programs whose purpose is to stress few aspects of a
system, here memory. The file \texttt{bench.c} is a simple micro
benchmark test file.
\Question $[$Optional$]$
Study the file, execute it, and deduce the cache hierarchy of your
system from the output. You should execute it several times to get
decent results.
\Answer .. % Your answer here.

\end{ExerciseList}

\newpage
\section{Authors}
I/We have solved these exercises independently, and each of us has actively
participated in the development of all of the exercise solutions.
\vspace{1cm}

\noindent
\begin{tabular}{p{70mm}p{70mm}}

%
% Replace ``Name x'' by your name.
%

Mads Vestergaard Carlsen & Rasmus Søgaard Jacobsen \\
\dots\dotfill\dots & \dots\dotfill\dots \\
Signature & Signature \\
& \\
& \\

Dan Duus Thøisen & Kristian Jensen \\
\dots\dotfill\dots & \dots\dotfill\dots \\
Signature & Signature \\
& \\
& \\
\end{tabular}

\end{document}
